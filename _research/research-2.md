---
title: "Multimodal Emotion Recognition using MELD Dataset"
collection: research
# category: projects
permalink: /research/meld-emotion-recognition
excerpt: 'Built and evaluated deep learning models to classify emotions from text, audio, and visual data. Implemented multiple fusion techniques—concatenation, Hadamard product, and cross-attention—to integrate modality embeddings. Trained models using the MELD dataset, with preprocessing pipelines for embedding extraction and formatting. Achieved strong performance across unimodal, bimodal, and trimodal emotion classification setups.'
# date: 2025-04-30
# venue: 'Stony Brook University'
projecturl: 'https://github.com/rajarshi-ray29/SceneSense'
image: /images/scenesense.png
---

### About the Project
This project explored **multimodal emotion recognition**, leveraging **text, audio, and visual data** to improve emotion classification accuracy.  
The research involved designing deep learning models that integrate embeddings across different modalities using advanced fusion techniques.

<p align="center">
  <img src="/images/scenesense2.png" alt="SceneSense Architecture" style="max-width: 800px; width: 100%; height: auto;">
</p>

---

### Key Contributions
- Designed and trained deep learning models for **emotion recognition** across **text, audio, and visual** modalities.  
- Implemented multiple **fusion strategies**: concatenation, Hadamard product, and cross-attention for modality integration.  
- Developed preprocessing pipelines for **embedding extraction and formatting** from multimodal inputs.  
- Evaluated models on **MELD dataset**, achieving strong performance across **unimodal, bimodal, and trimodal** setups.  

---

### Skills Applied
- **Multimodal AI**  
- **Natural Language Processing (NLP)**  
- **PyTorch**  
- **Artificial Intelligence (AI)**  

---

### Repository
[**GitHub – SceneSense**](https://github.com/rajarshi-ray29/SceneSense)  
